{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f707b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3f3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preperation \n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3dafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train = train['label']\n",
    "\n",
    "x_train = train.drop('label', axis=1)\n",
    "\n",
    "sns.countplot(y_train)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f85165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "x_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd972a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba078809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "x_train = x_train / 255\n",
    "test = test / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d930838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "# These 'images' are held in the dataframe with the shape 784,1 so to correctly load the images we need to reshape them into \n",
    "# 28x28 matrices. The 4th value in the reshape represents the number of 'channels' where 1 means black and white. 3 channels \n",
    "# would be RGB. \n",
    "\n",
    "x_train = x_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefa29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding \n",
    "y_train = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02540384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a013330",
   "metadata": {},
   "source": [
    "Now that we have completed the data preperation we will now build the CNN Model. We will be using a Keras Sequential Model. The first layer will have 32 filters and will use the variable Kernal_size which sections the image into evenly spaced parts, in this case (5,5 pixels). Each filter will look at a single kernal and will pass on the data to the next layer. we will have two of these layers  before moving on to 64 filter layers. \n",
    "\n",
    "Between these two sets of layers we will have a MaxPool2D layer which is called a pooling layer. This layer acts as a downsampling filter by looking at the two neighboring pixels and selecting the largest value. The size of the layer represents how large of an area in the image we want to 'search' in for the largest value. \n",
    "\n",
    "Dropout is a regularization method where a proportion of nodes in the layer are randomly ignored and have their weights set to 0 for the training sample. This randomly drops a proportion of the network and forces the learn the features in a new way. This technique reduces overfitting and improves generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5042ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Sequential()\n",
    "\n",
    "Model.add(Conv2D(filters =32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "Model.add(Conv2D(filters =32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "Model.add(MaxPool2D(pool_size=(2,2)))\n",
    "Model.add(Dropout(0.25))\n",
    "\n",
    "# Now we repeat with more filters\n",
    "\n",
    "Model.add(Conv2D(filters =64, kernel_size=(3,3), padding='same', activation='relu', ))\n",
    "Model.add(Conv2D(filters =64, kernel_size=(3,3), padding='same', activation='relu', ))\n",
    "\n",
    "Model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "Model.add(Dropout(0.25)) \n",
    "\n",
    "# To finish the model we use a flatten layer to convert the data into a single 1D vector. This step is needed to make full use \n",
    "# of the final fully connected layer. The last layer in the model has 10 nodes for each of the classification options and uses\n",
    "# softmax to return a distribution of probability for each class \n",
    "\n",
    "Model.add(Flatten())\n",
    "Model.add(Dense(256, activation='relu'))\n",
    "Model.add(Dropout(0.5))\n",
    "Model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dbf75",
   "metadata": {},
   "source": [
    "Now that we have set up the model we need to set up a score function, a loss function and a optimization algorithm. \n",
    "\n",
    "To measure the success of our model we use something called a loss function. This is the error rate between the observed labels and the predicted ones. We use a specific form for categorical classifications with greater than 2 classes called Categorical crossentropy. \n",
    "\n",
    "The optimizer is the most important aspect of this model. It will iteratively improce our parameters in order to minimize the loss. The optimizer I will be using is called RMSprop which adjusts the Adagrad method to reduce the learning rate. Another example of a good optimizer is the Stochastic Gradient Descent optimizer but that one is much slower. \n",
    "\n",
    "To be able to return a prediction we need to be able to evaluate our models predictions. We will evaluate on the metric 'accuracy' since we care about how accurate each prediction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f868d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compile the model!\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f2e9735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 86\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8530de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/313 - 32s - loss: 0.1048 - accuracy: 0.9691 - val_loss: 0.0654 - val_accuracy: 0.9792 - 32s/epoch - 103ms/step\n",
      "Epoch 2/30\n",
      "313/313 - 33s - loss: 0.0723 - accuracy: 0.9790 - val_loss: 0.0444 - val_accuracy: 0.9850 - 33s/epoch - 104ms/step\n",
      "Epoch 3/30\n",
      "313/313 - 33s - loss: 0.0559 - accuracy: 0.9827 - val_loss: 0.0491 - val_accuracy: 0.9851 - 33s/epoch - 106ms/step\n",
      "Epoch 4/30\n",
      "313/313 - 36s - loss: 0.0500 - accuracy: 0.9850 - val_loss: 0.0436 - val_accuracy: 0.9876 - 36s/epoch - 114ms/step\n",
      "Epoch 5/30\n",
      "313/313 - 35s - loss: 0.0410 - accuracy: 0.9874 - val_loss: 0.0433 - val_accuracy: 0.9878 - 35s/epoch - 110ms/step\n",
      "Epoch 6/30\n",
      "313/313 - 33s - loss: 0.0390 - accuracy: 0.9887 - val_loss: 0.0400 - val_accuracy: 0.9881 - 33s/epoch - 105ms/step\n",
      "Epoch 7/30\n",
      "313/313 - 32s - loss: 0.0341 - accuracy: 0.9900 - val_loss: 0.0392 - val_accuracy: 0.9875 - 32s/epoch - 103ms/step\n",
      "Epoch 8/30\n",
      "313/313 - 32s - loss: 0.0327 - accuracy: 0.9899 - val_loss: 0.0360 - val_accuracy: 0.9891 - 32s/epoch - 103ms/step\n",
      "Epoch 9/30\n",
      "313/313 - 32s - loss: 0.0300 - accuracy: 0.9910 - val_loss: 0.0337 - val_accuracy: 0.9908 - 32s/epoch - 102ms/step\n",
      "Epoch 10/30\n",
      "313/313 - 32s - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.0555 - val_accuracy: 0.9876 - 32s/epoch - 102ms/step\n",
      "Epoch 11/30\n",
      "313/313 - 32s - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.0442 - val_accuracy: 0.9884 - 32s/epoch - 102ms/step\n",
      "Epoch 12/30\n",
      "313/313 - 32s - loss: 0.0270 - accuracy: 0.9916 - val_loss: 0.0456 - val_accuracy: 0.9882 - 32s/epoch - 103ms/step\n",
      "Epoch 13/30\n",
      "313/313 - 32s - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.0322 - val_accuracy: 0.9914 - 32s/epoch - 102ms/step\n",
      "Epoch 14/30\n",
      "313/313 - 32s - loss: 0.0237 - accuracy: 0.9926 - val_loss: 0.0441 - val_accuracy: 0.9914 - 32s/epoch - 103ms/step\n",
      "Epoch 15/30\n",
      "313/313 - 32s - loss: 0.0210 - accuracy: 0.9937 - val_loss: 0.0500 - val_accuracy: 0.9888 - 32s/epoch - 103ms/step\n",
      "Epoch 16/30\n",
      "313/313 - 32s - loss: 0.0239 - accuracy: 0.9935 - val_loss: 0.0460 - val_accuracy: 0.9859 - 32s/epoch - 102ms/step\n",
      "Epoch 17/30\n",
      "313/313 - 32s - loss: 0.0235 - accuracy: 0.9932 - val_loss: 0.0419 - val_accuracy: 0.9899 - 32s/epoch - 103ms/step\n",
      "Epoch 18/30\n",
      "313/313 - 32s - loss: 0.0229 - accuracy: 0.9943 - val_loss: 0.0348 - val_accuracy: 0.9900 - 32s/epoch - 103ms/step\n",
      "Epoch 19/30\n",
      "313/313 - 32s - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.0328 - val_accuracy: 0.9936 - 32s/epoch - 102ms/step\n",
      "Epoch 20/30\n",
      "313/313 - 32s - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0350 - val_accuracy: 0.9899 - 32s/epoch - 103ms/step\n",
      "Epoch 21/30\n",
      "313/313 - 32s - loss: 0.0216 - accuracy: 0.9942 - val_loss: 0.0422 - val_accuracy: 0.9914 - 32s/epoch - 102ms/step\n",
      "Epoch 22/30\n",
      "313/313 - 32s - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.0364 - val_accuracy: 0.9923 - 32s/epoch - 102ms/step\n",
      "Epoch 23/30\n",
      "313/313 - 34s - loss: 0.0212 - accuracy: 0.9938 - val_loss: 0.0591 - val_accuracy: 0.9882 - 34s/epoch - 108ms/step\n",
      "Epoch 24/30\n",
      "313/313 - 35s - loss: 0.0198 - accuracy: 0.9946 - val_loss: 0.0408 - val_accuracy: 0.9894 - 35s/epoch - 113ms/step\n",
      "Epoch 25/30\n",
      "313/313 - 35s - loss: 0.0248 - accuracy: 0.9940 - val_loss: 0.0317 - val_accuracy: 0.9920 - 35s/epoch - 112ms/step\n",
      "Epoch 26/30\n",
      "313/313 - 36s - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.0570 - val_accuracy: 0.9902 - 36s/epoch - 116ms/step\n",
      "Epoch 27/30\n",
      "313/313 - 35s - loss: 0.0236 - accuracy: 0.9938 - val_loss: 0.0340 - val_accuracy: 0.9908 - 35s/epoch - 113ms/step\n",
      "Epoch 28/30\n",
      "313/313 - 36s - loss: 0.0204 - accuracy: 0.9940 - val_loss: 0.0476 - val_accuracy: 0.9914 - 36s/epoch - 115ms/step\n",
      "Epoch 29/30\n",
      "313/313 - 35s - loss: 0.0215 - accuracy: 0.9940 - val_loss: 0.0479 - val_accuracy: 0.9921 - 35s/epoch - 111ms/step\n",
      "Epoch 30/30\n",
      "313/313 - 35s - loss: 0.0239 - accuracy: 0.9935 - val_loss: 0.0383 - val_accuracy: 0.9912 - 35s/epoch - 112ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b802d2a70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed09405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 6s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict results\n",
    "results = Model.predict(test)\n",
    "\n",
    "# select the index with the maximum probability\n",
    "results = np.argmax(results, axis = 1)\n",
    "\n",
    "results = pd.Series(results, name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d86114b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a submission\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), results], axis = 1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
